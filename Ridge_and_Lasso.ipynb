{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to use Ridge and Lasso in Linear regression and compare Lasso and Ridge with standard regression\n",
    "The data source:\n",
    "https://github.com/learn-co-students/dsc-ridge-and-lasso-regression-lab-seattle-ds-062419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Prices Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Housing_Prices/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value = df.median(), inplace= True) # Impute null values: replace missing by the median per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure to remove the SalesPrice column from the predictors (which you store in `X`), \n",
    "X = df.drop(['SalePrice'], axis=1)\n",
    "#Store the target in `y`.\n",
    "y = df['SalePrice'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a selection of the data by removing some of the data with `dtype = object`, this way our first model only contains **continuous features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nonobj = X_train[[col for col,dtype in list(zip(X_train.columns, X_train.dtypes)) \n",
    "                          if dtype != np.dtype('O')]]\n",
    "X_test_nonobj = X_test[[col for col,dtype in list(zip(X_test.columns, X_test.dtypes)) \n",
    "                        if dtype != np.dtype('O')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use this data to perform a first naive linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the R squared and the MSE for both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and print R2 and MSE for train and test\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_nonobj, y_train)\n",
    "print(lr.score(X_train_nonobj, y_train))\n",
    "print(lr.score(X_test_nonobj, y_test))\n",
    "print(mean_squared_error(y_train, lr.predict(X_train_nonobj)))\n",
    "print(mean_squared_error(y_test, lr.predict(X_test_nonobj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From sklearn.preprocessing import StandardScaler to scale predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()  \n",
    "ss.fit(X_train_nonobj)\n",
    "# note that scaling is after train test split.\n",
    "X_train_nonobj_scaled = ss.transform(X_train_nonobj)\n",
    "#note that only X_train fit to ss not X_test\n",
    "X_test_nonobj_scaled = ss.transform(X_test_nonobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same linear regression on this data and print out R-squared and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_nonobj_scaled, y_train)\n",
    "preds = lr.predict(X_test_nonobj_scaled)\n",
    "\n",
    "print(lr.coef_)\n",
    "print(lr.score(X_train_nonobj_scaled, y_train))\n",
    "print(lr.score(X_test_nonobj_scaled, y_test))\n",
    "print(mean_squared_error(y_train, lr.predict(X_train_nonobj_scaled)))\n",
    "print(mean_squared_error(y_test, lr.predict(X_test_nonobj_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the \"object\" variables and do one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataframes with only the categorical variables\n",
    "\n",
    "X_train_obj = X_train[[col for col,dtype in list(zip(X_train.columns, X_train.dtypes)) \n",
    "                          if dtype == np.dtype('O')]]\n",
    "X_test_obj = X_test[[col for col,dtype in list(zip(X_test.columns, X_test.dtypes)) \n",
    "                        if dtype == np.dtype('O')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_obj = X_train_obj.fillna('NaN') \n",
    "X_test_obj = X_test_obj.fillna('NaN') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode....\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_obj_ohe = onehotencoder.fit_transform(X_train_obj)\n",
    "X_test_obj_ohe = onehotencoder.transform(X_test_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(X_test_obj_ohe.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge categorical dataframe with scaled `X` of noncategorical to have one predictor dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "X_train_scaled_all = pd.concat([pd.DataFrame(X_train_nonobj_scaled), pd.DataFrame(X_train_obj_ohe.todense())], axis=1)\n",
    "X_test_scaled_all = pd.concat([pd.DataFrame(X_test_nonobj_scaled), pd.DataFrame(X_test_obj_ohe.todense())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same linear regression on this data and print out R-squared and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled_all, y_train)\n",
    "preds = lr.predict(X_test_scaled_all)\n",
    "\n",
    "# print(lr.coef_)\n",
    "print(lr.score(X_train_scaled_all, y_train))\n",
    "print(lr.score(X_test_scaled_all, y_test))\n",
    "print(mean_squared_error(y_train, lr.predict(X_train_scaled_all)))\n",
    "print(mean_squared_error(y_test, lr.predict(X_test_scaled_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the severe overfitting above. Training R squared is very high, but the testing R squared is negative. predictions are not accurate and the scale of the test MSE is much higher than that of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Ridge and Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all the data (normalized features and categorical variables after onehotencoder) and perform Lasso and Ridge regression for both! Each time, look at R-squared and MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With default parameter (alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Lasso(alpha = 1) \n",
    "lasso.fit(X_train_scaled_all, y_train)\n",
    "print(lasso.score(X_train_scaled_all, y_train))\n",
    "print(lasso.score(X_test_scaled_all, y_test))\n",
    "print(mean_squared_error(y_train, lasso.predict(X_train_scaled_all)))\n",
    "print(mean_squared_error(y_test, lasso.predict(X_test_scaled_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linspace\n",
    "\n",
    "ls_alpha=[]\n",
    "MSE_trn= []\n",
    "MSE_tst= []\n",
    "for alpha in linspace(0.5,75.5,75):\n",
    "    ls_alpha.append(alpha)\n",
    "    lasso = Lasso(alpha = alpha) \n",
    "    lasso.fit(X_train_scaled_all, y_train)\n",
    "    MSE_trn.append(mean_squared_error(y_train, lasso.predict(X_train_scaled_all)))\n",
    "    MSE_tst.append(mean_squared_error(y_test, lasso.predict(X_test_scaled_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ls_alpha,MSE_trn)   \n",
    "plt.scatter(ls_alpha,MSE_tst);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that the test loss goes down while the train loss didn't change as much so the model is getting better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_MSE_tst = pd.Series(MSE_tst, index=ls_alpha)\n",
    "alpha_MSE_tst.argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At alpha of 33.945, loss of test set is minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With default parameter (alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "ridge = Ridge() #Lasso is also known as the L1 norm.\n",
    "ridge.fit(X_train_scaled_all, y_train)\n",
    "print(ridge.score(X_train_scaled_all, y_train))\n",
    "print(ridge.score(X_test_scaled_all, y_test))\n",
    "print(mean_squared_error(y_train, ridge.predict(X_train_scaled_all)))\n",
    "print(mean_squared_error(y_test, ridge.predict(X_test_scaled_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Ridge and Lasso parameter estimates that are very close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the total length of the parameter space and draw conclusions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Lasso params almost zero\n",
    "sum(abs(lasso.coef_) < 10**(-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of Ridge params almost zero\n",
    "\n",
    "sum(abs(ridge.coef_) < 10**(-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(lasso.coef_) < 10**(-10))/len(lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was initially overfit to train. Once I did regularization and decreased the effect of some coefficients that weren't really serving, I moved away from overfitting and made my predictions way better. Lasso was effective to essentially perform variable selection and remove about 12% of the variables from your model with a default alpha. However, to get better regularization, I need to play around with alpha."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
